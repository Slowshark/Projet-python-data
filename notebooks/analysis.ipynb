{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6bd525",
   "metadata": {},
   "source": [
    "# Analyse des Vulnérabilités ANSSI\n",
    "## Extraction, Enrichissement et Machine Learning\n",
    "\n",
    "Ce notebook couvre l'ensemble du pipeline d'analyse des vulnérabilités ANSSI:\n",
    "- Extraction des flux RSS\n",
    "- Enrichissement des CVE avec APIs externes\n",
    "- Consolidation dans un DataFrame\n",
    "- Analyses exploratoires et visualisations\n",
    "- Modèles de Machine Learning (supervisé et non-supervisé)\n",
    "- Génération d'alertes personnalisées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b196786",
   "metadata": {},
   "source": [
    "## 1. Import des Bibliothèques Requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulations de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Paramètres de visualisation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Toutes les bibliothèques importées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade33a68",
   "metadata": {},
   "source": [
    "## 2. Chargement du DataFrame Consolidé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du chemin\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Import du chargeur de données locales\n",
    "from src.local_data_loader import LocalDataLoader\n",
    "\n",
    "# Chargement des données depuis data4project\n",
    "loader = LocalDataLoader(\"../data4project\")\n",
    "df = loader.load_all(mitre_limit=None, avis_limit=None)  # Charger toutes les données\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"✓ DataFrame chargé: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "    print(f\"✓ CVE uniques: {df['cve_id'].nunique()}\")\n",
    "    print(f\"✓ Alertes: {len(df[df['type_bulletin'] == 'alerte'])}\")\n",
    "    print(f\"✓ Avis: {len(df[df['type_bulletin'] == 'avis'])}\")\n",
    "else:\n",
    "    print(\"✗ Aucune donnée chargée\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12416b61",
   "metadata": {},
   "source": [
    "## 3. Exploration des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== Informations Générales ===\")\n",
    "    print(f\"Dimensions: {df.shape}\")\n",
    "    print(f\"\\nColonnes:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nValeurs manquantes:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"\\nPremières lignes:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "if df is not None:\n",
    "    print(\"=== Statistiques Descriptives ===\")\n",
    "    print(df.describe())\n",
    "    print(f\"\\n=== Statistiques par Catégorie ===\")\n",
    "    print(f\"CVE uniques: {df['cve_id'].nunique()}\")\n",
    "    print(f\"Bulletins uniques: {df['id_anssi'].nunique()}\")\n",
    "    print(f\"Vendors uniques: {df['vendor'].nunique()}\")\n",
    "    print(f\"Produits uniques: {df['produit'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24766f6d",
   "metadata": {},
   "source": [
    "## 3.1 Visualisation des Données Manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs manquantes\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Pourcentage de valeurs manquantes par colonne\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=True)\n",
    "    colors = ['#2ecc71' if x < 20 else '#f39c12' if x < 50 else '#e74c3c' for x in missing_pct.values]\n",
    "    \n",
    "    axes[0].barh(missing_pct.index, missing_pct.values, color=colors)\n",
    "    axes[0].set_xlabel('Pourcentage de valeurs manquantes (%)')\n",
    "    axes[0].set_title('Complétude des Données par Colonne')\n",
    "    axes[0].axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50%')\n",
    "    \n",
    "    # Heatmap des valeurs manquantes (échantillon)\n",
    "    sample_size = min(100, len(df))\n",
    "    missing_matrix = df.sample(sample_size).isnull().astype(int)\n",
    "    sns.heatmap(missing_matrix.T, cbar=False, cmap='YlOrRd', ax=axes[1])\n",
    "    axes[1].set_xlabel('Échantillon de lignes')\n",
    "    axes[1].set_ylabel('Colonnes')\n",
    "    axes[1].set_title('Pattern des Données Manquantes (échantillon)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Résumé\n",
    "    print(f\"\\n=== Résumé Complétude ===\")\n",
    "    print(f\"Colonnes complètes: {(missing_pct == 0).sum()}\")\n",
    "    print(f\"Colonnes > 50% manquantes: {(missing_pct > 50).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2497f46",
   "metadata": {},
   "source": [
    "## 3.2 Analyse Temporelle des Vulnérabilités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d43f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse temporelle des CVE\n",
    "if df is not None and 'date_published' in df.columns:\n",
    "    df_time = df.dropna(subset=['date_published']).copy()\n",
    "    # Conversion explicite en DatetimeIndex pour éviter les erreurs de type Pylance\n",
    "    df_time['date_published'] = pd.to_datetime(df_time['date_published'])\n",
    "    date_idx = pd.DatetimeIndex(df_time['date_published'])\n",
    "    df_time['year'] = date_idx.year\n",
    "    df_time['month'] = date_idx.month\n",
    "    df_time['year_month'] = date_idx.to_period('M')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Évolution annuelle des CVE\n",
    "    yearly_counts = df_time.groupby('year').size()\n",
    "    axes[0, 0].bar(yearly_counts.index.astype(int), yearly_counts.values, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Année')\n",
    "    axes[0, 0].set_ylabel('Nombre de CVE')\n",
    "    axes[0, 0].set_title('Évolution Annuelle des Vulnérabilités')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Score CVSS moyen par année\n",
    "    yearly_cvss = df_time.groupby('year')['cvss_score'].mean()\n",
    "    axes[0, 1].plot(yearly_cvss.index.astype(int), yearly_cvss.values, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].fill_between(yearly_cvss.index.astype(int), yearly_cvss.values, alpha=0.3, color='red')\n",
    "    axes[0, 1].set_xlabel('Année')\n",
    "    axes[0, 1].set_ylabel('Score CVSS Moyen')\n",
    "    axes[0, 1].set_title('Évolution du Score CVSS Moyen par Année')\n",
    "    axes[0, 1].set_ylim(0, 10)\n",
    "    axes[0, 1].axhline(y=7, color='orange', linestyle='--', alpha=0.7, label='Seuil Élevé (7)')\n",
    "    axes[0, 1].axhline(y=9, color='red', linestyle='--', alpha=0.7, label='Seuil Critique (9)')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Distribution mensuelle\n",
    "    monthly_counts = df_time.groupby('month').size()\n",
    "    month_names = ['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Jun', 'Jul', 'Aoû', 'Sep', 'Oct', 'Nov', 'Déc']\n",
    "    axes[1, 0].bar(range(1, 13), [monthly_counts.get(i, 0) for i in range(1, 13)], color='coral', edgecolor='black')\n",
    "    axes[1, 0].set_xticks(range(1, 13))\n",
    "    axes[1, 0].set_xticklabels(month_names, rotation=45)\n",
    "    axes[1, 0].set_xlabel('Mois')\n",
    "    axes[1, 0].set_ylabel('Nombre de CVE')\n",
    "    axes[1, 0].set_title('Distribution Mensuelle des CVE (tous les ans)')\n",
    "    \n",
    "    # 4. Sévérité par année (stacked bar)\n",
    "    if 'base_severity' in df_time.columns:\n",
    "        severity_by_year = df_time.groupby(['year', 'base_severity']).size().unstack(fill_value=0)\n",
    "        severity_colors = {'CRITICAL': '#d32f2f', 'HIGH': '#f57c00', 'MEDIUM': '#fbc02d', 'LOW': '#388e3c'}\n",
    "        severity_by_year.plot(kind='bar', stacked=True, ax=axes[1, 1], \n",
    "                              color=[severity_colors.get(col, 'gray') for col in severity_by_year.columns])\n",
    "        axes[1, 1].set_xlabel('Année')\n",
    "        axes[1, 1].set_ylabel('Nombre de CVE')\n",
    "        axes[1, 1].set_title('Sévérité des CVE par Année')\n",
    "        axes[1, 1].legend(title='Sévérité', bbox_to_anchor=(1.02, 1))\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Statistiques Temporelles ===\")\n",
    "    print(f\"Période: {df_time['year'].min()} - {df_time['year'].max()}\")\n",
    "    print(f\"Total CVE avec date: {len(df_time)}\")\n",
    "else:\n",
    "    print(\"Pas de données temporelles disponibles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812942af",
   "metadata": {},
   "source": [
    "## 4. Visualisations Exploratoires - Scores CVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda089f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "if df is not None:\n",
    "    df_clean = df.dropna(subset=['cvss_score'])\n",
    "    \n",
    "    # Histogramme CVSS\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Distribution des scores CVSS\n",
    "    axes[0, 0].hist(df_clean['cvss_score'], bins=20, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Score CVSS')\n",
    "    axes[0, 0].set_ylabel('Fréquence')\n",
    "    axes[0, 0].set_title('Distribution des Scores CVSS')\n",
    "    axes[0, 0].axvline(df_clean['cvss_score'].mean(), color='red', linestyle='--', label=f\"Moyenne: {df_clean['cvss_score'].mean():.2f}\")\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Gravité par Score\n",
    "    severity_counts = df_clean['base_severity'].value_counts()\n",
    "    axes[0, 1].bar(severity_counts.index, severity_counts.values, color=['#d32f2f', '#f57c00', '#fbc02d', '#388e3c'])\n",
    "    axes[0, 1].set_xlabel('Sévérité')\n",
    "    axes[0, 1].set_ylabel('Nombre')\n",
    "    axes[0, 1].set_title('Distribution de la Sévérité')\n",
    "    \n",
    "    # 3. Box plot CVSS par type de bulletin\n",
    "    df_clean.boxplot(column='cvss_score', by='type_bulletin', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Type de Bulletin')\n",
    "    axes[1, 0].set_ylabel('Score CVSS')\n",
    "    axes[1, 0].set_title('Distribution CVSS par Type de Bulletin')\n",
    "    plt.sca(axes[1, 0])\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 4. Score EPSS\n",
    "    df_clean_epss = df.dropna(subset=['epss_score'])\n",
    "    axes[1, 1].hist(df_clean_epss['epss_score'], bins=20, color='coral', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Score EPSS')\n",
    "    axes[1, 1].set_ylabel('Fréquence')\n",
    "    axes[1, 1].set_title('Distribution des Scores EPSS')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773a589",
   "metadata": {},
   "source": [
    "## 5. Visualisations - Types de Vulnérabilités (CWE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Top 10 CWE\n",
    "    top_cwe = df['cwe_id'].value_counts().head(10)\n",
    "    \n",
    "    fig = px.pie(\n",
    "        values=top_cwe.values,\n",
    "        names=top_cwe.index,\n",
    "        title='Top 10 Types de Vulnérabilités (CWE)',\n",
    "        labels=top_cwe.index\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 CWE:\")\n",
    "    print(top_cwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212c02e",
   "metadata": {},
   "source": [
    "## 6. Corrélation CVSS-EPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    df_corr = df.dropna(subset=['cvss_score', 'epss_score'])\n",
    "    \n",
    "    # Scatter plot\n",
    "    fig = px.scatter(\n",
    "        df_corr,\n",
    "        x='cvss_score',\n",
    "        y='epss_score',\n",
    "        color='base_severity',\n",
    "        size='epss_score',\n",
    "        hover_data=['cve_id', 'vendor', 'produit'],\n",
    "        title='Corrélation entre CVSS et EPSS',\n",
    "        labels={'cvss_score': 'Score CVSS', 'epss_score': 'Score EPSS'}\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Matrice de corrélation\n",
    "    print(f\"\\nCorrélation CVSS-EPSS: {df_corr['cvss_score'].corr(df_corr['epss_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef9fc6",
   "metadata": {},
   "source": [
    "## 7. Vendors et Produits les Plus Impactés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38af7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Top vendors\n",
    "    top_vendors = df['vendor'].value_counts().head(10)\n",
    "    axes[0].barh(top_vendors.index, top_vendors.values, color='steelblue')\n",
    "    axes[0].set_xlabel('Nombre de Vulnérabilités')\n",
    "    axes[0].set_title('Top 10 Éditeurs Affectés')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Top produits\n",
    "    top_produits = df['produit'].value_counts().head(10)\n",
    "    axes[1].barh(top_produits.index, top_produits.values, color='coral')\n",
    "    axes[1].set_xlabel('Nombre de Vulnérabilités')\n",
    "    axes[1].set_title('Top 10 Produits Affectés')\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f919e14",
   "metadata": {},
   "source": [
    "## 7.1 Analyse Interactive des Vendors (Treemap et Sunburst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f851521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treemap interactif des vendors et produits\n",
    "if df is not None:\n",
    "    # Préparation des données pour le treemap\n",
    "    vendor_product = df.groupby(['vendor', 'produit']).agg({\n",
    "        'cve_id': 'count',\n",
    "        'cvss_score': 'mean'\n",
    "    }).reset_index()\n",
    "    vendor_product.columns = ['vendor', 'produit', 'count', 'cvss_moyen']\n",
    "    vendor_product = vendor_product[vendor_product['vendor'] != '']\n",
    "    \n",
    "    # Top 50 combinaisons vendor-produit\n",
    "    vendor_product_top = vendor_product.nlargest(50, 'count')\n",
    "    \n",
    "    # Treemap\n",
    "    fig = px.treemap(\n",
    "        vendor_product_top,\n",
    "        path=['vendor', 'produit'],\n",
    "        values='count',\n",
    "        color='cvss_moyen',\n",
    "        color_continuous_scale='RdYlGn_r',\n",
    "        title='Treemap: Vendors et Produits les Plus Vulnérables',\n",
    "        hover_data={'cvss_moyen': ':.2f'}\n",
    "    )\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # Sunburst chart\n",
    "    fig2 = px.sunburst(\n",
    "        vendor_product_top,\n",
    "        path=['vendor', 'produit'],\n",
    "        values='count',\n",
    "        color='cvss_moyen',\n",
    "        color_continuous_scale='RdYlGn_r',\n",
    "        title='Sunburst: Hiérarchie Vendors/Produits'\n",
    "    )\n",
    "    fig2.update_layout(height=600)\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90ac9e",
   "metadata": {},
   "source": [
    "## 7.2 Heatmap: CVSS par Vendor et Type de Vulnérabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e57414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap CVSS par vendor et CWE\n",
    "if df is not None:\n",
    "    # Top vendors et CWE\n",
    "    top_vendors = df['vendor'].value_counts().head(15).index.tolist()\n",
    "    top_cwe = df['cwe_id'].value_counts().head(10).index.tolist()\n",
    "    \n",
    "    df_heatmap = df[(df['vendor'].isin(top_vendors)) & (df['cwe_id'].isin(top_cwe))]\n",
    "    \n",
    "    if not df_heatmap.empty:\n",
    "        # Pivot table pour la heatmap\n",
    "        pivot = df_heatmap.pivot_table(\n",
    "            values='cvss_score',\n",
    "            index='vendor',\n",
    "            columns='cwe_id',\n",
    "            aggfunc='mean'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn_r', \n",
    "                    linewidths=0.5, vmin=0, vmax=10,\n",
    "                    cbar_kws={'label': 'Score CVSS Moyen'})\n",
    "        plt.xlabel('Type de Vulnérabilité (CWE)')\n",
    "        plt.ylabel('Vendor')\n",
    "        plt.title('Score CVSS Moyen par Vendor et Type de Vulnérabilité')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyse radar pour top 5 vendors\n",
    "        fig = px.line_polar(\n",
    "            df_heatmap.groupby('vendor')['cvss_score'].mean().reset_index().nlargest(8, 'cvss_score'),\n",
    "            r='cvss_score',\n",
    "            theta='vendor',\n",
    "            line_close=True,\n",
    "            title='Radar: Score CVSS Moyen des Top Vendors'\n",
    "        )\n",
    "        fig.update_traces(fill='toself')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41419c33",
   "metadata": {},
   "source": [
    "## 7.3 Analyse des Risques (CVSS vs EPSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de risque CVSS vs EPSS\n",
    "if df is not None:\n",
    "    df_risk = df.dropna(subset=['cvss_score', 'epss_score']).copy()\n",
    "    \n",
    "    if not df_risk.empty:\n",
    "        # Catégorisation des risques\n",
    "        def categorize_risk(row):\n",
    "            cvss = row['cvss_score']\n",
    "            epss = row['epss_score']\n",
    "            if cvss >= 7 and epss >= 0.5:\n",
    "                return 'CRITIQUE - Priorité 1'\n",
    "            elif cvss >= 7 or epss >= 0.5:\n",
    "                return 'ÉLEVÉ - Priorité 2'\n",
    "            elif cvss >= 4 or epss >= 0.1:\n",
    "                return 'MOYEN - Priorité 3'\n",
    "            else:\n",
    "                return 'FAIBLE - Priorité 4'\n",
    "        \n",
    "        df_risk['risk_category'] = df_risk.apply(categorize_risk, axis=1)\n",
    "        \n",
    "        # Scatter plot avec zones de risque\n",
    "        fig = px.scatter(\n",
    "            df_risk,\n",
    "            x='cvss_score',\n",
    "            y='epss_score',\n",
    "            color='risk_category',\n",
    "            hover_data=['cve_id', 'vendor', 'produit'],\n",
    "            color_discrete_map={\n",
    "                'CRITIQUE - Priorité 1': '#d32f2f',\n",
    "                'ÉLEVÉ - Priorité 2': '#f57c00',\n",
    "                'MOYEN - Priorité 3': '#fbc02d',\n",
    "                'FAIBLE - Priorité 4': '#388e3c'\n",
    "            },\n",
    "            title='Matrice de Risque: CVSS vs Probabilité d\\'Exploitation (EPSS)',\n",
    "            labels={'cvss_score': 'Score CVSS (Sévérité)', 'epss_score': 'Score EPSS (Probabilité)'}\n",
    "        )\n",
    "        \n",
    "        # Ajout des lignes de seuil\n",
    "        fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"Seuil EPSS 50%\")\n",
    "        fig.add_vline(x=7, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"Seuil CVSS 7\")\n",
    "        \n",
    "        fig.update_layout(height=600)\n",
    "        fig.show()\n",
    "        \n",
    "        # Distribution des catégories de risque\n",
    "        risk_counts = df_risk['risk_category'].value_counts()\n",
    "        fig2 = px.bar(\n",
    "            x=risk_counts.index,\n",
    "            y=risk_counts.values,\n",
    "            color=risk_counts.index,\n",
    "            color_discrete_map={\n",
    "                'CRITIQUE - Priorité 1': '#d32f2f',\n",
    "                'ÉLEVÉ - Priorité 2': '#f57c00',\n",
    "                'MOYEN - Priorité 3': '#fbc02d',\n",
    "                'FAIBLE - Priorité 4': '#388e3c'\n",
    "            },\n",
    "            title='Distribution des Catégories de Risque',\n",
    "            labels={'x': 'Catégorie', 'y': 'Nombre de CVE'}\n",
    "        )\n",
    "        fig2.show()\n",
    "        \n",
    "        print(f\"\\n=== Analyse des Risques ===\")\n",
    "        print(risk_counts.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a05f2",
   "metadata": {},
   "source": [
    "## 8. Machine Learning - Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b721d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Création d'une copie pour le ML\n",
    "    df_ml = df.copy()\n",
    "    \n",
    "    # Remplissage des valeurs manquantes\n",
    "    df_ml['cvss_score'].fillna(df_ml['cvss_score'].median(), inplace=True)\n",
    "    df_ml['epss_score'].fillna(df_ml['epss_score'].median(), inplace=True)\n",
    "    \n",
    "    # Création de la variable cible: criticité (basée sur CVSS)\n",
    "    def get_criticality(cvss):\n",
    "        if pd.isna(cvss):\n",
    "            return 'Unknown'\n",
    "        elif cvss >= 9:\n",
    "            return 'Critique'\n",
    "        elif cvss >= 7:\n",
    "            return 'Élevée'\n",
    "        elif cvss >= 4:\n",
    "            return 'Moyenne'\n",
    "        else:\n",
    "            return 'Faible'\n",
    "    \n",
    "    df_ml['criticality'] = df_ml['cvss_score'].apply(get_criticality)\n",
    "    \n",
    "    # Encodage des variables catégorielles\n",
    "    le_vendor = LabelEncoder()\n",
    "    le_product = LabelEncoder()\n",
    "    le_severity = LabelEncoder()\n",
    "    le_criticality = LabelEncoder()\n",
    "    \n",
    "    df_ml['vendor_encoded'] = le_vendor.fit_transform(df_ml['vendor'].fillna('Unknown'))\n",
    "    df_ml['produit_encoded'] = le_product.fit_transform(df_ml['produit'].fillna('Unknown'))\n",
    "    df_ml['severity_encoded'] = le_severity.fit_transform(df_ml['base_severity'].fillna('Unknown'))\n",
    "    df_ml['criticality_encoded'] = le_criticality.fit_transform(df_ml['criticality'])\n",
    "    \n",
    "    print(\"✓ Données préparées pour ML\")\n",
    "    print(f\"Criticité: {df_ml['criticality'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9825f76",
   "metadata": {},
   "source": [
    "## 9. Modèle Non-Supervisé: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation des features\n",
    "    features_clustering = df_ml[['cvss_score', 'epss_score', 'vendor_encoded', 'severity_encoded']].copy()\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_clustering)\n",
    "    \n",
    "    # Détermination du nombre optimal de clusters (Elbow method)\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(features_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(features_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    axes[0].plot(K_range, inertias, 'bo-')\n",
    "    axes[0].set_xlabel('Nombre de Clusters')\n",
    "    axes[0].set_ylabel('Inertie')\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "    axes[1].set_xlabel('Nombre de Clusters')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Analysis')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sélection du meilleur k\n",
    "    best_k = K_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"✓ Meilleur nombre de clusters: {best_k} (Silhouette: {max(silhouette_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f216e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # K-Means avec k optimal\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    df_ml['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    # Visualisation avec PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n",
    "                         c=df_ml['cluster'], cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0],\n",
    "               pca.transform(kmeans.cluster_centers_)[:, 1],\n",
    "               marker='+', s=300, c='red', edgecolors='black', linewidth=2,\n",
    "               label='Centroïdes')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    plt.title('K-Means Clustering (PCA Visualisation)')\n",
    "    plt.legend()\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse des clusters\n",
    "    print(f\"\\nDistribution des clusters:\")\n",
    "    print(df_ml['cluster'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nCaractéristiques des clusters:\")\n",
    "    cluster_analysis = df_ml.groupby('cluster')[['cvss_score', 'epss_score']].mean()\n",
    "    print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665d7df",
   "metadata": {},
   "source": [
    "## 10. Modèle Supervisé: Classification de la Criticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation des features pour la classification\n",
    "    features_sup = df_ml[['cvss_score', 'epss_score', 'vendor_encoded', 'produit_encoded', 'severity_encoded']].copy()\n",
    "    target = df_ml['criticality_encoded']\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_sup, target, test_size=0.2, random_state=42, stratify=target\n",
    "    )\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler_sup = StandardScaler()\n",
    "    X_train_scaled = scaler_sup.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_sup.transform(X_test)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Évaluation\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"=== Modèle de Classification - Random Forest ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"\\nRapport de Classification:\")\n",
    "    criticality_labels = le_criticality.classes_\n",
    "    print(classification_report(y_test, y_pred, target_names=criticality_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(criticality_labels),\n",
    "                yticklabels=list(criticality_labels))\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Réalité')\n",
    "    plt.title('Matrice de Confusion - Classification Criticité')\n",
    "    plt.show()\n",
    "    \n",
    "    # Importance des features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features_sup.columns,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Importance des Features - Classification')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nImportance des Features:\")\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d501a8",
   "metadata": {},
   "source": [
    "## 11. Modèle Supervisé: Prédiction du Score EPSS (Régression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation pour la régression\n",
    "    df_regression = df_ml.dropna(subset=['epss_score']).copy()\n",
    "    \n",
    "    features_reg = df_regression[['cvss_score', 'vendor_encoded', 'produit_encoded', 'severity_encoded']]\n",
    "    target_reg = df_regression['epss_score']\n",
    "    \n",
    "    # Split\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "        features_reg, target_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler_reg = StandardScaler()\n",
    "    X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "    X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "    \n",
    "    # Entraînement\n",
    "    regressor = GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)\n",
    "    regressor.fit(X_train_reg_scaled, y_train_reg)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_reg = regressor.predict(X_test_reg_scaled)\n",
    "    \n",
    "    # Évaluation\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = regressor.score(X_test_reg_scaled, y_test_reg)\n",
    "    \n",
    "    print(\"=== Modèle de Régression - Gradient Boosting ===\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "    plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "            [y_test_reg.min(), y_test_reg.max()], \n",
    "            'r--', lw=2)\n",
    "    plt.xlabel('EPSS Réel')\n",
    "    plt.ylabel('EPSS Prédis')\n",
    "    plt.title(f'Prédiction EPSS (R²={r2:.4f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6435d1",
   "metadata": {},
   "source": [
    "## 12. Validation des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== Résumé de la Validation des Modèles ===\")\n",
    "    print(f\"\\n1. Clustering (K-Means):\")\n",
    "    print(f\"   - Nombre de clusters: {best_k}\")\n",
    "    print(f\"   - Silhouette Score: {max(silhouette_scores):.4f}\")\n",
    "    print(f\"   - Variance expliquée (PCA): {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Classification Criticité (Random Forest):\")\n",
    "    print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   - F1-Score: {f1:.4f}\")\n",
    "    print(f\"   - Samples: {len(X_test)}\")\n",
    "    \n",
    "    print(f\"\\n3. Régression EPSS (Gradient Boosting):\")\n",
    "    print(f\"   - R² Score: {r2:.4f}\")\n",
    "    print(f\"   - RMSE: {rmse:.4f}\")\n",
    "    print(f\"   - Samples: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683ac9a",
   "metadata": {},
   "source": [
    "## 13. Génération d'Alertes Personnalisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Définition des règles d'alerte\n",
    "    def create_alert_level(row) -> str:\n",
    "        cvss = row['cvss_score']\n",
    "        epss = row['epss_score']\n",
    "        \n",
    "        if pd.isna(cvss):\n",
    "            return ''\n",
    "        \n",
    "        # Critique\n",
    "        if cvss >= 9:\n",
    "            return 'CRITIQUE'\n",
    "        if cvss >= 7 and pd.notna(epss) and epss >= 0.75:\n",
    "            return 'CRITIQUE'\n",
    "        \n",
    "        # Élevée\n",
    "        if cvss >= 7:\n",
    "            return 'ÉLEVÉE'\n",
    "        if epss and epss >= 0.75:\n",
    "            return 'ÉLEVÉE'\n",
    "        \n",
    "        # Moyenne\n",
    "        if cvss >= 4:\n",
    "            return 'MOYENNE'\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    df['alert_level'] = df.apply(create_alert_level, axis=1)  # type: ignore[call-overload]\n",
    "    \n",
    "    # Statistiques d'alertes\n",
    "    print(\"=== Statistiques d'Alertes ===\")\n",
    "    alert_counts = df['alert_level'].value_counts()\n",
    "    print(alert_counts)\n",
    "    \n",
    "    # Visualisation\n",
    "    fig = px.pie(\n",
    "        values=alert_counts.values,\n",
    "        names=alert_counts.index,\n",
    "        color=alert_counts.index,\n",
    "        color_discrete_map={'CRITIQUE': '#d32f2f', 'ÉLEVÉE': '#f57c00', 'MOYENNE': '#fbc02d'},\n",
    "        title='Distribution des Niveaux d\\'Alerte'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Top alertes\n",
    "    print(f\"\\n=== Top 5 Alertes Critiques ===\")\n",
    "    critical_alerts = df[df['alert_level'] == 'CRITIQUE'].sort_values('cvss_score', ascending=False).head(5)\n",
    "    for idx, row in critical_alerts.iterrows():\n",
    "        print(f\"\\n{row['cve_id']} - {row['produit']} ({row['vendor']})\")\n",
    "        print(f\"  CVSS: {row['cvss_score']}, EPSS: {row['epss_score']}\")\n",
    "        print(f\"  Bulletin: {row['titre_anssi']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5d3cb",
   "metadata": {},
   "source": [
    "## 14. Résumé et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a56d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== RÉSUMÉ DU PROJET ===\")\n",
    "    print(f\"\\n✓ Données consolidées: {len(df)} lignes\")\n",
    "    print(f\"✓ CVE uniques: {df['cve_id'].nunique()}\")\n",
    "    print(f\"✓ Bulletins uniques: {df['id_anssi'].nunique()}\")\n",
    "    print(f\"✓ Vendors couverts: {df['vendor'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\n✓ Modèles développés:\")\n",
    "    print(f\"  1. K-Means Clustering ({best_k} clusters)\")\n",
    "    print(f\"  2. Classification Criticité (Accuracy: {accuracy:.2%})\")\n",
    "    print(f\"  3. Régression EPSS (R²: {r2:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Alertes générées:\")\n",
    "    for level, count in alert_counts.items():\n",
    "        print(f\"  - {level}: {count}\")\n",
    "    \n",
    "    print(f\"\\n✓ Livrables créés:\")\n",
    "    print(f\"  - data/processed/cves_consolidated.csv\")\n",
    "    print(f\"  - output/alerts/\")\n",
    "    print(f\"  - notebooks/analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235311dd",
   "metadata": {},
   "source": [
    "## 14.1 Dashboard de Synthèse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard de synthèse avec indicateurs clés (KPIs)\n",
    "if df is not None:\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Calcul des KPIs\n",
    "    total_cve = df['cve_id'].nunique()\n",
    "    total_bulletins = df['id_anssi'].nunique()\n",
    "    cvss_mean = df['cvss_score'].mean()\n",
    "    epss_mean = df['epss_score'].mean()\n",
    "    critical_count = len(df[df['cvss_score'] >= 9])\n",
    "    high_count = len(df[(df['cvss_score'] >= 7) & (df['cvss_score'] < 9)])\n",
    "    \n",
    "    # Création du dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        specs=[\n",
    "            [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}]\n",
    "        ],\n",
    "        subplot_titles=(\"\", \"\", \"\", \"Répartition par Sévérité\", \"Top 5 Vendors\", \"CVSS vs EPSS\")\n",
    "    )\n",
    "    \n",
    "    # KPIs\n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number+delta\",\n",
    "        value=total_cve,\n",
    "        title={\"text\": \"CVE Uniques\"},\n",
    "        domain={'x': [0, 0.33], 'y': [0.6, 1]}\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\",\n",
    "        value=round(cvss_mean, 2) if pd.notna(cvss_mean) else 0,\n",
    "        title={\"text\": \"Score CVSS Moyen\"},\n",
    "        number={'suffix': \"/10\"},\n",
    "        domain={'x': [0.33, 0.66], 'y': [0.6, 1]}\n",
    "    ), row=1, col=2)\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\",\n",
    "        value=critical_count,\n",
    "        title={\"text\": \"CVE Critiques (≥9)\"},\n",
    "        number={'font': {'color': 'red'}},\n",
    "        domain={'x': [0.66, 1], 'y': [0.6, 1]}\n",
    "    ), row=1, col=3)\n",
    "    \n",
    "    # Pie chart sévérité\n",
    "    severity_counts = df['base_severity'].value_counts()\n",
    "    fig.add_trace(go.Pie(\n",
    "        labels=severity_counts.index,\n",
    "        values=severity_counts.values,\n",
    "        marker_colors=['#d32f2f', '#f57c00', '#fbc02d', '#388e3c', '#9e9e9e']\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    # Bar chart top vendors\n",
    "    top_5_vendors = df['vendor'].value_counts().head(5)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=top_5_vendors.index,\n",
    "        y=top_5_vendors.values,\n",
    "        marker_color='steelblue'\n",
    "    ), row=2, col=2)\n",
    "    \n",
    "    # Scatter CVSS vs EPSS\n",
    "    df_scatter = df.dropna(subset=['cvss_score', 'epss_score']).sample(min(500, len(df)))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_scatter['cvss_score'],\n",
    "        y=df_scatter['epss_score'],\n",
    "        mode='markers',\n",
    "        marker=dict(color=df_scatter['cvss_score'], colorscale='RdYlGn_r', size=6),\n",
    "        text=df_scatter['cve_id']\n",
    "    ), row=2, col=3)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        title_text=\"Dashboard de Synthèse - Analyse des Vulnérabilités\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Sauvegarde des données\n",
    "    output_path = \"../data/processed/cves_consolidated_local.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Données sauvegardées: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}