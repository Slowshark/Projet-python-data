{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6bd525",
   "metadata": {},
   "source": [
    "# Analyse des Vulnérabilités ANSSI\n",
    "## Extraction, Enrichissement et Machine Learning\n",
    "\n",
    "Ce notebook couvre l'ensemble du pipeline d'analyse des vulnérabilités ANSSI:\n",
    "- Extraction des flux RSS\n",
    "- Enrichissement des CVE avec APIs externes\n",
    "- Consolidation dans un DataFrame\n",
    "- Analyses exploratoires et visualisations\n",
    "- Modèles de Machine Learning (supervisé et non-supervisé)\n",
    "- Génération d'alertes personnalisées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b196786",
   "metadata": {},
   "source": [
    "## 1. Import des Bibliothèques Requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulations de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Paramètres de visualisation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Toutes les bibliothèques importées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade33a68",
   "metadata": {},
   "source": [
    "## 2. Chargement du DataFrame Consolidé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du fichier CSV\n",
    "csv_path = \"data/processed/cves_consolidated.csv\"\n",
    "\n",
    "# Vérification du fichier\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"✓ DataFrame chargé: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "else:\n",
    "    print(f\"✗ Fichier non trouvé: {csv_path}\")\n",
    "    print(\"Exécutez d'abord main.py pour générer le fichier\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12416b61",
   "metadata": {},
   "source": [
    "## 3. Exploration des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== Informations Générales ===\")\n",
    "    print(f\"Dimensions: {df.shape}\")\n",
    "    print(f\"\\nColonnes:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nValeurs manquantes:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"\\nPremières lignes:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "if df is not None:\n",
    "    print(\"=== Statistiques Descriptives ===\")\n",
    "    print(df.describe())\n",
    "    print(f\"\\n=== Statistiques par Catégorie ===\")\n",
    "    print(f\"CVE uniques: {df['cve_id'].nunique()}\")\n",
    "    print(f\"Bulletins uniques: {df['id_anssi'].nunique()}\")\n",
    "    print(f\"Vendors uniques: {df['vendor'].nunique()}\")\n",
    "    print(f\"Produits uniques: {df['produit'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812942af",
   "metadata": {},
   "source": [
    "## 4. Visualisations Exploratoires - Scores CVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda089f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "if df is not None:\n",
    "    df_clean = df.dropna(subset=['cvss_score'])\n",
    "    \n",
    "    # Histogramme CVSS\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Distribution des scores CVSS\n",
    "    axes[0, 0].hist(df_clean['cvss_score'], bins=20, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Score CVSS')\n",
    "    axes[0, 0].set_ylabel('Fréquence')\n",
    "    axes[0, 0].set_title('Distribution des Scores CVSS')\n",
    "    axes[0, 0].axvline(df_clean['cvss_score'].mean(), color='red', linestyle='--', label=f\"Moyenne: {df_clean['cvss_score'].mean():.2f}\")\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Gravité par Score\n",
    "    severity_counts = df_clean['base_severity'].value_counts()\n",
    "    axes[0, 1].bar(severity_counts.index, severity_counts.values, color=['#d32f2f', '#f57c00', '#fbc02d', '#388e3c'])\n",
    "    axes[0, 1].set_xlabel('Sévérité')\n",
    "    axes[0, 1].set_ylabel('Nombre')\n",
    "    axes[0, 1].set_title('Distribution de la Sévérité')\n",
    "    \n",
    "    # 3. Box plot CVSS par type de bulletin\n",
    "    df_clean.boxplot(column='cvss_score', by='type_bulletin', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Type de Bulletin')\n",
    "    axes[1, 0].set_ylabel('Score CVSS')\n",
    "    axes[1, 0].set_title('Distribution CVSS par Type de Bulletin')\n",
    "    plt.sca(axes[1, 0])\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 4. Score EPSS\n",
    "    df_clean_epss = df.dropna(subset=['epss_score'])\n",
    "    axes[1, 1].hist(df_clean_epss['epss_score'], bins=20, color='coral', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Score EPSS')\n",
    "    axes[1, 1].set_ylabel('Fréquence')\n",
    "    axes[1, 1].set_title('Distribution des Scores EPSS')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773a589",
   "metadata": {},
   "source": [
    "## 5. Visualisations - Types de Vulnérabilités (CWE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Top 10 CWE\n",
    "    top_cwe = df['cwe_id'].value_counts().head(10)\n",
    "    \n",
    "    fig = px.pie(\n",
    "        values=top_cwe.values,\n",
    "        names=top_cwe.index,\n",
    "        title='Top 10 Types de Vulnérabilités (CWE)',\n",
    "        labels=top_cwe.index\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 CWE:\")\n",
    "    print(top_cwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212c02e",
   "metadata": {},
   "source": [
    "## 6. Corrélation CVSS-EPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    df_corr = df.dropna(subset=['cvss_score', 'epss_score'])\n",
    "    \n",
    "    # Scatter plot\n",
    "    fig = px.scatter(\n",
    "        df_corr,\n",
    "        x='cvss_score',\n",
    "        y='epss_score',\n",
    "        color='base_severity',\n",
    "        size='epss_score',\n",
    "        hover_data=['cve_id', 'vendor', 'produit'],\n",
    "        title='Corrélation entre CVSS et EPSS',\n",
    "        labels={'cvss_score': 'Score CVSS', 'epss_score': 'Score EPSS'}\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Matrice de corrélation\n",
    "    print(f\"\\nCorrélation CVSS-EPSS: {df_corr['cvss_score'].corr(df_corr['epss_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef9fc6",
   "metadata": {},
   "source": [
    "## 7. Vendors et Produits les Plus Impactés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38af7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Top vendors\n",
    "    top_vendors = df['vendor'].value_counts().head(10)\n",
    "    axes[0].barh(top_vendors.index, top_vendors.values, color='steelblue')\n",
    "    axes[0].set_xlabel('Nombre de Vulnérabilités')\n",
    "    axes[0].set_title('Top 10 Éditeurs Affectés')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Top produits\n",
    "    top_produits = df['produit'].value_counts().head(10)\n",
    "    axes[1].barh(top_produits.index, top_produits.values, color='coral')\n",
    "    axes[1].set_xlabel('Nombre de Vulnérabilités')\n",
    "    axes[1].set_title('Top 10 Produits Affectés')\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a05f2",
   "metadata": {},
   "source": [
    "## 8. Machine Learning - Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b721d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Création d'une copie pour le ML\n",
    "    df_ml = df.copy()\n",
    "    \n",
    "    # Remplissage des valeurs manquantes\n",
    "    df_ml['cvss_score'].fillna(df_ml['cvss_score'].median(), inplace=True)\n",
    "    df_ml['epss_score'].fillna(df_ml['epss_score'].median(), inplace=True)\n",
    "    \n",
    "    # Création de la variable cible: criticité (basée sur CVSS)\n",
    "    def get_criticality(cvss):\n",
    "        if pd.isna(cvss):\n",
    "            return 'Unknown'\n",
    "        elif cvss >= 9:\n",
    "            return 'Critique'\n",
    "        elif cvss >= 7:\n",
    "            return 'Élevée'\n",
    "        elif cvss >= 4:\n",
    "            return 'Moyenne'\n",
    "        else:\n",
    "            return 'Faible'\n",
    "    \n",
    "    df_ml['criticality'] = df_ml['cvss_score'].apply(get_criticality)\n",
    "    \n",
    "    # Encodage des variables catégorielles\n",
    "    le_vendor = LabelEncoder()\n",
    "    le_product = LabelEncoder()\n",
    "    le_severity = LabelEncoder()\n",
    "    le_criticality = LabelEncoder()\n",
    "    \n",
    "    df_ml['vendor_encoded'] = le_vendor.fit_transform(df_ml['vendor'].fillna('Unknown'))\n",
    "    df_ml['produit_encoded'] = le_product.fit_transform(df_ml['produit'].fillna('Unknown'))\n",
    "    df_ml['severity_encoded'] = le_severity.fit_transform(df_ml['base_severity'].fillna('Unknown'))\n",
    "    df_ml['criticality_encoded'] = le_criticality.fit_transform(df_ml['criticality'])\n",
    "    \n",
    "    print(\"✓ Données préparées pour ML\")\n",
    "    print(f\"Criticité: {df_ml['criticality'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9825f76",
   "metadata": {},
   "source": [
    "## 9. Modèle Non-Supervisé: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation des features\n",
    "    features_clustering = df_ml[['cvss_score', 'epss_score', 'vendor_encoded', 'severity_encoded']].copy()\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_clustering)\n",
    "    \n",
    "    # Détermination du nombre optimal de clusters (Elbow method)\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(features_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(features_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    axes[0].plot(K_range, inertias, 'bo-')\n",
    "    axes[0].set_xlabel('Nombre de Clusters')\n",
    "    axes[0].set_ylabel('Inertie')\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "    axes[1].set_xlabel('Nombre de Clusters')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Analysis')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sélection du meilleur k\n",
    "    best_k = K_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"✓ Meilleur nombre de clusters: {best_k} (Silhouette: {max(silhouette_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f216e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # K-Means avec k optimal\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    df_ml['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    # Visualisation avec PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n",
    "                         c=df_ml['cluster'], cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0],\n",
    "               pca.transform(kmeans.cluster_centers_)[:, 1],\n",
    "               marker='+', s=300, c='red', edgecolors='black', linewidth=2,\n",
    "               label='Centroïdes')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    plt.title('K-Means Clustering (PCA Visualisation)')\n",
    "    plt.legend()\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse des clusters\n",
    "    print(f\"\\nDistribution des clusters:\")\n",
    "    print(df_ml['cluster'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nCaractéristiques des clusters:\")\n",
    "    cluster_analysis = df_ml.groupby('cluster')[['cvss_score', 'epss_score']].mean()\n",
    "    print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665d7df",
   "metadata": {},
   "source": [
    "## 10. Modèle Supervisé: Classification de la Criticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation des features pour la classification\n",
    "    features_sup = df_ml[['cvss_score', 'epss_score', 'vendor_encoded', 'produit_encoded', 'severity_encoded']].copy()\n",
    "    target = df_ml['criticality_encoded']\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_sup, target, test_size=0.2, random_state=42, stratify=target\n",
    "    )\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler_sup = StandardScaler()\n",
    "    X_train_scaled = scaler_sup.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_sup.transform(X_test)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Évaluation\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"=== Modèle de Classification - Random Forest ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"\\nRapport de Classification:\")\n",
    "    criticality_labels = le_criticality.classes_\n",
    "    print(classification_report(y_test, y_pred, target_names=criticality_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(criticality_labels),\n",
    "                yticklabels=list(criticality_labels))\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Réalité')\n",
    "    plt.title('Matrice de Confusion - Classification Criticité')\n",
    "    plt.show()\n",
    "    \n",
    "    # Importance des features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features_sup.columns,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Importance des Features - Classification')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nImportance des Features:\")\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d501a8",
   "metadata": {},
   "source": [
    "## 11. Modèle Supervisé: Prédiction du Score EPSS (Régression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Préparation pour la régression\n",
    "    df_regression = df_ml.dropna(subset=['epss_score']).copy()\n",
    "    \n",
    "    features_reg = df_regression[['cvss_score', 'vendor_encoded', 'produit_encoded', 'severity_encoded']]\n",
    "    target_reg = df_regression['epss_score']\n",
    "    \n",
    "    # Split\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "        features_reg, target_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler_reg = StandardScaler()\n",
    "    X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "    X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "    \n",
    "    # Entraînement\n",
    "    regressor = GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)\n",
    "    regressor.fit(X_train_reg_scaled, y_train_reg)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_reg = regressor.predict(X_test_reg_scaled)\n",
    "    \n",
    "    # Évaluation\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = regressor.score(X_test_reg_scaled, y_test_reg)\n",
    "    \n",
    "    print(\"=== Modèle de Régression - Gradient Boosting ===\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "    plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "            [y_test_reg.min(), y_test_reg.max()], \n",
    "            'r--', lw=2)\n",
    "    plt.xlabel('EPSS Réel')\n",
    "    plt.ylabel('EPSS Prédis')\n",
    "    plt.title(f'Prédiction EPSS (R²={r2:.4f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6435d1",
   "metadata": {},
   "source": [
    "## 12. Validation des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== Résumé de la Validation des Modèles ===\")\n",
    "    print(f\"\\n1. Clustering (K-Means):\")\n",
    "    print(f\"   - Nombre de clusters: {best_k}\")\n",
    "    print(f\"   - Silhouette Score: {max(silhouette_scores):.4f}\")\n",
    "    print(f\"   - Variance expliquée (PCA): {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Classification Criticité (Random Forest):\")\n",
    "    print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   - F1-Score: {f1:.4f}\")\n",
    "    print(f\"   - Samples: {len(X_test)}\")\n",
    "    \n",
    "    print(f\"\\n3. Régression EPSS (Gradient Boosting):\")\n",
    "    print(f\"   - R² Score: {r2:.4f}\")\n",
    "    print(f\"   - RMSE: {rmse:.4f}\")\n",
    "    print(f\"   - Samples: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683ac9a",
   "metadata": {},
   "source": [
    "## 13. Génération d'Alertes Personnalisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Définition des règles d'alerte\n",
    "    def create_alert_level(row) -> str:\n",
    "        cvss = row['cvss_score']\n",
    "        epss = row['epss_score']\n",
    "        \n",
    "        if pd.isna(cvss):\n",
    "            return ''\n",
    "        \n",
    "        # Critique\n",
    "        if cvss >= 9:\n",
    "            return 'CRITIQUE'\n",
    "        if cvss >= 7 and pd.notna(epss) and epss >= 0.75:\n",
    "            return 'CRITIQUE'\n",
    "        \n",
    "        # Élevée\n",
    "        if cvss >= 7:\n",
    "            return 'ÉLEVÉE'\n",
    "        if epss and epss >= 0.75:\n",
    "            return 'ÉLEVÉE'\n",
    "        \n",
    "        # Moyenne\n",
    "        if cvss >= 4:\n",
    "            return 'MOYENNE'\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    df['alert_level'] = df.apply(create_alert_level, axis=1)  # type: ignore[call-overload]\n",
    "    \n",
    "    # Statistiques d'alertes\n",
    "    print(\"=== Statistiques d'Alertes ===\")\n",
    "    alert_counts = df['alert_level'].value_counts()\n",
    "    print(alert_counts)\n",
    "    \n",
    "    # Visualisation\n",
    "    fig = px.pie(\n",
    "        values=alert_counts.values,\n",
    "        names=alert_counts.index,\n",
    "        color=alert_counts.index,\n",
    "        color_discrete_map={'CRITIQUE': '#d32f2f', 'ÉLEVÉE': '#f57c00', 'MOYENNE': '#fbc02d'},\n",
    "        title='Distribution des Niveaux d\\'Alerte'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Top alertes\n",
    "    print(f\"\\n=== Top 5 Alertes Critiques ===\")\n",
    "    critical_alerts = df[df['alert_level'] == 'CRITIQUE'].sort_values('cvss_score', ascending=False).head(5)\n",
    "    for idx, row in critical_alerts.iterrows():\n",
    "        print(f\"\\n{row['cve_id']} - {row['produit']} ({row['vendor']})\")\n",
    "        print(f\"  CVSS: {row['cvss_score']}, EPSS: {row['epss_score']}\")\n",
    "        print(f\"  Bulletin: {row['titre_anssi']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5d3cb",
   "metadata": {},
   "source": [
    "## 14. Résumé et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a56d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== RÉSUMÉ DU PROJET ===\")\n",
    "    print(f\"\\n✓ Données consolidées: {len(df)} lignes\")\n",
    "    print(f\"✓ CVE uniques: {df['cve_id'].nunique()}\")\n",
    "    print(f\"✓ Bulletins uniques: {df['id_anssi'].nunique()}\")\n",
    "    print(f\"✓ Vendors couverts: {df['vendor'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\n✓ Modèles développés:\")\n",
    "    print(f\"  1. K-Means Clustering ({best_k} clusters)\")\n",
    "    print(f\"  2. Classification Criticité (Accuracy: {accuracy:.2%})\")\n",
    "    print(f\"  3. Régression EPSS (R²: {r2:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Alertes générées:\")\n",
    "    for level, count in alert_counts.items():\n",
    "        print(f\"  - {level}: {count}\")\n",
    "    \n",
    "    print(f\"\\n✓ Livrables créés:\")\n",
    "    print(f\"  - data/processed/cves_consolidated.csv\")\n",
    "    print(f\"  - output/alerts/\")\n",
    "    print(f\"  - notebooks/analysis.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
